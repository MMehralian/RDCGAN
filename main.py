import os,sys,pprint,time
import numpy as np
import tensorflow as tf
import tensorlayer as tl
from tensorlayer.layers import *
pp = pprint.PrettyPrinter()
from model import *

flags = tf.app.flags
flags.DEFINE_integer("epoch",100,"Epoch to train [25]")
flags.DEFINE_float("learning_rate",0.0002,"Learning rate for optimizer [0.0002]")
flags.DEFINE_float("beta1",0.5,"Momentum term for optimizer [0.5]")
flags.DEFINE_integer("train_size",60000,"The size of train images [np.inf]")
flags.DEFINE_integer("batch_size",64,"The number of batch images [64]")
flags.DEFINE_integer("output_size",28,"The size of the output images to generate [32]")
flags.DEFINE_integer("z_dim",2,"dimention for noise vector")
flags.DEFINE_integer("sample_size",64,"The numbers of sample images [64]")
flags.DEFINE_integer("c_dim",1,"Diminsion of image color [3]")
flags.DEFINE_integer("sample_step",500,"The interval of generating sample. [500]")
flags.DEFINE_integer("save_step",500,"The interval of saving checkpoints. [500]")
flags.DEFINE_string("dataset","mnist","The name of dataset [mnist,svhn,cifar10,image_net]")
flags.DEFINE_integer("no_classes",10,"The number of class for Fully Connected networks [10]")
flags.DEFINE_string("sample_dir", "samples/mnist_MSE_z2_2by2_(thesis4)", "Directory name to save the image samples [samples]")
flags.DEFINE_string("checkpoint_dir","checkpoint_CIFAR/mnist_MSE_z2_2by2_(thesis4)","Directory name to save the checkpoints [checkpoint]")
flags.DEFINE_boolean("is_train",True,"True for training, False for testing")
flags.DEFINE_boolean("visualize",False,"True for Visualizing, False for nothing")
FLAGS = flags.FLAGS

def main(_):
    pp.pprint(flags.FLAGS.__flags)
    tl.files.exists_or_mkdir(FLAGS.checkpoint_dir)
    tl.files.exists_or_mkdir(FLAGS.sample_dir)
    # preparing data
    # for SVHN your dataset should be an .mat file
    datasetPath = "insert your dataset path here"
    if FLAGS.dataset == "mnist":
        X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1),
                                                                                     path=datasetPath)
        X_train = X_train[:,:,:,]
        X_train = (X_train / .5) - 1;
        X_test = (X_test / .5) - 1
    if FLAGS.dataset == "cifar10":
        X_train, y_train, X_test, y_test = tl.files.load_cifar10_dataset(shape=(-1, 32, 32, 3),
                                                                         path=datasetPath)
        X_train = (X_train / 127.5) - 1;X_test = (X_test / 127.5) - 1
        X_val = X_test[0:5000, :];X_test = X_test[5000:10000, :]
        y_val = y_test[0:5000];y_test = y_test[5000:10000]
    if FLAGS.dataset == 'svhn':
        import scipy.io
        mat_train = scipy.io.loadmat(datasetPath+"/train.mat")
        mat_test = scipy.io.loadmat(datasetPath+"/test.mat")
        X_train = mat_train['X'];X_test = mat_test['X']
        y_train = mat_train['y']-1;y_test = mat_test['y']-1
        X_train = (X_train / 127.5) - 1;X_test = (X_test / 127.5) - 1

        X_train = np.transpose(X_train,(3,0,1,2))
        X_test = np.transpose(X_test, (3,0,1,2))
        X_val = X_test[0:5000, :];X_test = X_test[5000:, :]
        y_val = y_test[0:5000];y_test = y_test[5000:]

    with tf.device("/gpu:0"):
        ##================ Define Placeholders ================##
        z = tf.placeholder(tf.float32,shape=[FLAGS.batch_size,FLAGS.z_dim], name = 'Z_noise')
        x = tf.placeholder(tf.float32,shape=[FLAGS.batch_size,FLAGS.output_size,FLAGS.output_size,FLAGS.c_dim],name='real_images')
        y_ = tf.placeholder(tf.int64, shape=[FLAGS.batch_size,FLAGS.no_classes], name='y_')

        # z--> generator for training (generating fake images): G(z)
        net_g, g_logits = Generator(z,is_train=True,reuse=False)

        # fake images --> discriminator: D(G(z))
        net_d, d_logits,d_fc = Discriminator(net_g.outputs,is_train=True,reuse=False)

        net_e, e_lofits, _ = Encoder(x,is_train=True, reuse=False) # for model 2

        # encode_image --> generator: G(E(x))
        net_gE, gE_logits = Generator(net_e.outputs,is_train=True,reuse=True)

        # fake image which generated by autoencoder: D(G(E(x)))
        net_dE,dE_logits,d_fc = Discriminator(net_gE.outputs,is_train=True,reuse=True)
        # real images --> discriminator: D(x)
        net_d2,d2_logits,d_fc = Discriminator(x,is_train=True,reuse=True)

        # z --> generate for evaluation, is_train = False
        net_g2,g2_logits = Generator(z,is_train=False,reuse=True)

        ##================ Define Train operations ================##
        # cost for updating discriminator and generator and encoder
        # encoder: try to encode images
        # MSE is calculated from real image x and G(E(x)): ||x-G(E(X)))||
        #Reconstruction L2 loss
        #L2_loss = 0.00005 * tf.nn.l2_loss(x-net_gE.outputs)
        MSE = np.sum((net_gE.outputs - x)**2)
        MSE = tf.reshape(MSE,shape=[FLAGS.batch_size,FLAGS.output_size*FLAGS.output_size*FLAGS.c_dim])
        MSE = tf.reduce_mean(tf.reduce_sum(MSE,1),name='error')
        # discriminator: real's; label = 1
        d_loss_real = tl.cost.sigmoid_cross_entropy(d2_logits,tf.ones_like(d2_logits), name='d_real')
        # discriminator: images from generator D(G(z)); label = 0
        d_loss_fake = tl.cost.sigmoid_cross_entropy(d_logits,tf.zeros_like(d_logits),name='d_fake')
        d_loss = d_loss_real + d_loss_fake
        d_loss_encode = tl.cost.sigmoid_cross_entropy(dE_logits,tf.ones_like(dE_logits),name='d_encode')

        # encoders loss
        e_loss = d_loss_encode + MSE
        # generator: try to make the fake images; label = 1
        g_loss = tl.cost.sigmoid_cross_entropy(d_logits,tf.ones_like(d_logits),name='g_fake')+e_loss


        g_vars = tl.layers.get_variables_with_name('generator',True,True)
        d_vars = tl.layers.get_variables_with_name('discriminator',True,True)
        e_vars = tl.layers.get_variables_with_name('encoder',True,True)

        net_g.print_params(False)
        print("---------------")
        net_d.print_params(False)
        print("---------------")
        net_e.print_params(False)

        # optimizers for updating discriminator and generator
        d_optim = tf.train.AdamOptimizer(FLAGS.learning_rate, beta1=FLAGS.beta1) \
                          .minimize(d_loss, var_list=d_vars)
        g_optim = tf.train.AdamOptimizer(FLAGS.learning_rate, beta1=FLAGS.beta1,) \
                          .minimize(g_loss, var_list=g_vars)
        e_optim = tf.train.AdamOptimizer(FLAGS.learning_rate, beta1=FLAGS.beta1) \
                          .minimize(e_loss,var_list=e_vars)
    summary_op = tf.summary.merge_all()
    sess = tf.InteractiveSession()
    tl.layers.initialize_global_variables(sess)

    model_dir = "%s_%s_%s" %(FLAGS.dataset,FLAGS.batch_size,FLAGS.output_size)
    save_dir = os.path.join(FLAGS.checkpoint_dir, model_dir)
    tl.files.exists_or_mkdir(FLAGS.checkpoint_dir, model_dir)
    tl.files.exists_or_mkdir(save_dir)
    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(FLAGS.sample_size, FLAGS.z_dim)).astype(
        np.float32)  # sample_seed = np.random.uniform(low=-1, high=1, size=(FLAGS.sample_size, z_dim)).astype(np.float32)

    #sample_seed = np.random.uniform(-1,1,size=(FLAGS.sample_size, FLAGS.z_dim)).astype(np.float32)
    #load the latest checkpoints
    net_g_name = os.path.join(save_dir,'net_g.npz')
    net_d_name = os.path.join(save_dir,'net_d.npz')
    net_e_name = os.path.join(save_dir, 'net_e.npz')

    ##================ Restore models ================##
    '''para_E = tl.files.load_npz(name='net_e.npz')
    tl.files.assign_params(sess, para_E, net_e)
    para_D = tl.files.load_npz(name='net_d.npz')
    tl.files.assign_params(sess, para_D, net_d2)
    para_G = tl.files.load_npz(name='net_g.npz')
    tl.files.assign_params(sess, para_G, net_g)'''
    ##================ Train models ================##
    iter_counter = 0
    batch_idx = int(len(X_train)/FLAGS.batch_size)
    #logs_path = "/tmp/svhn"
    #writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())
    for epoch in range(FLAGS.epoch):
        for idx in range(0,batch_idx):
            batch_images = X_train[idx*FLAGS.batch_size:(idx+1)*FLAGS.batch_size,:]
            batch_z = np.random.normal(loc=0.0,scale=1.0,size=(FLAGS.sample_size,FLAGS.z_dim)).astype(np.float32)
            #batch_z = np.random.uniform(-1,1,size=(FLAGS.sample_size, FLAGS.z_dim)).astype(np.float32)
            start_time = time.time()
            #update the discriminator
            errD,_ = sess.run([d_loss,d_optim],feed_dict={z:batch_z,x:batch_images})
            #errE,_ = sess.run([e_loss,e_optim],feed_dict={x:batch_images})
            for _ in range(2):
                errE, _ = sess.run([e_loss,e_optim], feed_dict={z: batch_z, x: batch_images})
                #summary = sess.run(summary_op)
                #writer.add_summary(summary, epoch * 100 + idx)
                errG,_ = sess.run([g_loss,g_optim],feed_dict={z:batch_z,x:batch_images})
            print("Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %8f, g_loss: %8f, e_loss: %8f" \
                 % (epoch, FLAGS.epoch, idx, batch_idx, time.time() - start_time, errD, errG,errE))
            iter_counter+=1
            if iter_counter % FLAGS.sample_step == 0:
                #generate and visualize fake images
                img_fake,img_AE,errD,errG,errE = sess.run([net_g2.outputs,net_gE.outputs,d_loss,g_loss,e_loss],feed_dict={z:sample_seed,x:batch_images}) #batch_images can be modified
                tl.visualize.save_images(img_fake,[8,8],'./{}/{:02d}_{:04d}_train(fake).png'.format(FLAGS.sample_dir,epoch,idx))
                tl.visualize.save_images(img_AE,[8,8],
                                         './{}/{:02d}_{:04d}_train(autoencoder).png'.format(FLAGS.sample_dir, epoch, idx))
                tl.visualize.save_images(batch_images, [8, 8],
                                         './{}/{:02d}_{:04d}_train(dataset).png'.format(FLAGS.sample_dir, epoch,idx))
                print("[sample] d_loss: %.8f, g_loss: %.8f, e_loss: %.8f" %(errD,errG,errE))

            if iter_counter % FLAGS.save_step ==0:
                #saver parameters
                print("[*] Saving checkpoints...")
                tl.files.save_npz(net_g.all_params, name = net_g_name,sess=sess)
                tl.files.save_npz(net_d.all_params,name= net_d_name,sess=sess)
                tl.files.save_npz(net_e.all_params,name= net_e_name,sess=sess)
                print("[*] Saving checkpoints SUCCESS")

if __name__ == '__main__':
    tf.app.run()